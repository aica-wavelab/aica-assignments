{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melody Generation\n",
    "\n",
    "+ **AI in Culture and Arts - Tech Crash Course**\n",
    "+ **Date:** 06.06.2024\n",
    "+ **Author:** B. ZÃ¶nnchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/aica-wavelab/aica-assignments/blob/main/A4_melody_generation/4_1_ml_ffn_markov.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will create music sheets and sound. For those tasks ``Python`` requires external programs that you should install if you are working locally:\n",
    "\n",
    "1. [Musescore](https://musescore.org/de) (for generating sheets)\n",
    "2. [FluidSynth](https://www.fluidsynth.org/) (for generating sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on google ``Colab``, you can evaluate the following to cells to install these applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title install dependencies to play sound\n",
    "%%capture\n",
    "print('installing fluidsynth...')\n",
    "!apt-get install fluidsynth > /dev/null\n",
    "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title install dependencies to show score in music notation\n",
    "%%capture\n",
    "print('installing musescore3...')\n",
    "!apt-get install musescore3 > /dev/null\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title clone git repository\n",
    "%%capture\n",
    "!rm -rf musical-interrogation\n",
    "!git clone https://github.com/aica-wavelab/aica-assignments.git\n",
    "%cd A4_melody_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furtheremore, for this notebook we need the following ``Python`` packages and moduls. Execute the cell to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install music21\n",
    "%pip install pyfluidsynth\n",
    "\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"4_1_ml_ffn_markov.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import music21 as m21\n",
    "from music21.stream import Part, Score\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pianoroll import stream_to_df, plot_df\n",
    "from encoder import NoteToIntEncoder, TERM_SYMBOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deep Learning for Melody Generation\n",
    "\n",
    "## 4.1 Learning the Markov Matrix\n",
    "\n",
    "In the following, we will use our first ``tensorflow`` model which consists of a single quadratic matrix $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$. This matrix conains all **learnable** parameters, meaning that the values of the elements of $\\mathbf{W}$ changes during *training*. The goal is that given a *one-hot encoded* note/event $\\mathbf{x} \\in \\mathbf{R}^{n}$, we can predict the next note/event by evaluating\n",
    "\n",
    "$$\\mathbf{x}^{\\top} \\mathbf{W}.$$\n",
    "\n",
    "Since $\\mathbf{x}$ is *one-hot encoded* this operation just gives us the $j^{\\text{th}}$ row of the matix $\\mathbf{W}$. A row tells us which event/note comes next. As we will see, the row is in fact assumed to be the natural logarithm of a discrete probability distribution. Computing $\\exp(w_{ij})$ for each element of $\\mathbf{W}$ gives us the probabilities similar to our Markov matrix $\\mathbf{M}$.\n",
    "\n",
    "Why is this the case? Well the reason for this can be found if we look at the *loss function* of the model.\n",
    "Suppose the model predicts a certain discrete probability distribution.\n",
    "And let us say that $p$ is the probability (computed by the model) of the correct prediction (according to our example provided to the model!).\n",
    "We want this probability to be rather high thus the loss it causes rather low if its value is high.\n",
    "Therefore $-p$ is a good loss for this single value.\n",
    "\n",
    "Let's assume we have multiple predictions. And let $p_1, \\ldots, p_m$ be the probabilities of the correct labels, then a reasonable loss would be the *negative likelyhood*:\n",
    "\n",
    "$$L = - \\left(p_1 \\cdot \\ldots \\cdot p_m \\right).$$\n",
    "\n",
    "However, we can make things easier by computing the *negative log-likelyhood*. By doing so we can replaces multiplication by addition:\n",
    "\n",
    "$$\\ln(L) = -\\left(\\ln(p_1) + \\ldots + \\ln(p_m)\\right) = -\\ln(p_1) - \\ldots - \\ln(p_m).$$\n",
    "\n",
    "So, to make things easier we think of the entries in $\\mathbf{W}$ as the *negative log-probability*!\n",
    "\n",
    "We use the same training data we used for learning a *Markov matrix* $\\mathbf{M}$ and we will see that, after training, $\\mathbf{M}$ and $\\mathbf{W}$ are suspiciously similar! \n",
    "\n",
    "**Disclaimer:** This notebook consists intentionally of very low-level code. Take you time and ask the coaches about what is going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us load our simple melody\n",
    "bach_minuet = m21.converter.parse('data/Minuet_in_G.mid')\n",
    "bach_minuet_melody = bach_minuet.parts[0]\n",
    "bach_minuet_melody.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the melody to get a in a *piano roll representation* to get a feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = stream_to_df(bach_minuet_melody)\n",
    "plot_df(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our Markov example, we transform the ``Stream`` into a list of numbers ranging from $0$ to $n-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate our encoder\n",
    "encoder = NoteToIntEncoder([bach_minuet_melody])\n",
    "\n",
    "# encode the Stream into a squence of numbers\n",
    "enc_bach_minuet = encoder.encode_sequence(bach_minuet_melody)\n",
    "\n",
    "# add terminal symbol to indicate the begining and end of the piece\n",
    "enc_bach_minuet = [encoder.encode(TERM_SYMBOL)] + enc_bach_minuet + [encoder.encode(TERM_SYMBOL)]\n",
    "\n",
    "print(enc_bach_minuet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify some (hyper-)parameters. Note that the learning rate is unusually large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(encoder) # all MIDI keys\n",
    "batch_size = 32\n",
    "learning_rate = 1.0\n",
    "epochs = 500\n",
    "\n",
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to generate our training data set. Each row of the input matrix $\\mathbf{X}$ represents one hot-encoded note/event while the (output/labels) vector $\\mathbf{y}$ is not one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(enc_bach_minuet)-1):\n",
    "  current_event = enc_bach_minuet[i]\n",
    "  next_event = enc_bach_minuet[i+1]\n",
    "  X.append(current_event)\n",
    "  y.append(next_event)\n",
    "\n",
    "X = tf.one_hot(X, depth=vocab_size).numpy().astype('float32')\n",
    "y = np.array(y)\n",
    "\n",
    "print(X[:3])\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass is calculated as follows: First, we multiply the two matrices \n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{X} \\cdot \\mathbf{W},$$\n",
    "\n",
    "with $\\mathbf{W} \\in \\mathbf{R}^{n \\times n}, \\mathbf{X} \\in \\mathbf{R}^{m \\times n}$ and $\\mathbf{C} \\in \\mathbf{R}^{m \\times n}.$\n",
    "\n",
    "Then, we perform the so-called *softmax* operation **row-wise**:\n",
    "\n",
    "$$p_{ki} = \\frac{\\exp(c_{ki})}{\\sum_{j=1} \\exp(c_{kj}) } \\text{ for all } k,i = 1, \\ldots, n.$$\n",
    "\n",
    "In other words, we **normalize** each row after applying the exponential function component-wise. Each row of the resulting matrix $\\mathbf{P}$ can thus be interpreted as a probability distribution!\n",
    "\n",
    "For optimization using *gradient descent*, we need an appropriate *cost function*/*loss function* $L$.\n",
    "To get it, we consider the \"probability\" for all correctly chosen transitions, i.e., the *likelihood*.\n",
    "Let $p_1, \\ldots, p_n$ be these probabilities (one per transition, aka row in $\\mathbf{X}$), then\n",
    "\n",
    "$$p_{1} \\cdot \\ldots \\cdot p_m$$\n",
    "\n",
    "is the *likelihood*.\n",
    "However, to be able to add instead, we compute the *negative log-likelihood*:\n",
    "\n",
    "$$-(\\log(p_1) + \\ldots + \\log(p_m)).$$\n",
    "\n",
    "By calling ``loss.backward()``, *backpropagation* (also known as the *backward pass*) is performed, and we can update our weights by\n",
    "\n",
    "$$\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\cdot \\nabla_\\mathbf{W} L $$\n",
    "\n",
    "In our case, we choose a very large *learning rate* $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W = tf.Variable(tf.random.normal((vocab_size, vocab_size)), dtype=tf.float32)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "learning_rate = 10.0\n",
    "\n",
    "for k in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. Forward pass\n",
    "        C = tf.matmul(X, W)\n",
    "        C = tf.exp(C)\n",
    "        P = C / tf.reduce_sum(C, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -tf.reduce_mean(tf.math.log(tf.gather(P, y, batch_dims=1)))\n",
    "    \n",
    "    if k % 100 == 0:\n",
    "        print(f'Epoch {k}, Loss: {loss.numpy()}')\n",
    "    \n",
    "    # 2. Backward pass\n",
    "    grads = tape.gradient(loss, [W])\n",
    "    \n",
    "    # 3. Update weights\n",
    "    W.assign_sub(learning_rate * grads[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compre the matrix $\\mathbf{W}$ with our Markov matrix $\\mathbf{M}$ from the previous section.\n",
    "Let's recompute $\\mathbf{M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_matrix(parts, encoder):\n",
    "  m = len(encoder)\n",
    "\n",
    "  # initiate the Markov matrix with zeros\n",
    "  M = np.zeros((m,m))\n",
    "\n",
    "  # count the transitions; a row represents the starting point and the column the end point\n",
    "  for part in parts:\n",
    "    predecessor = TERM_SYMBOL\n",
    "    idx2 = 0\n",
    "    for element in part.recurse():\n",
    "      if isinstance(element, (m21.note.Rest, m21.note.Note)):\n",
    "        idx1 = encoder.encode(predecessor)\n",
    "        idx2 = encoder.encode(element)\n",
    "        M[idx1][idx2] += 1.0\n",
    "        predecessor = element\n",
    "      # this is for the ending since we arrive at a TERM_SYMBOL\n",
    "    M[idx2][encoder.encode(TERM_SYMBOL)] += 1.0\n",
    "\n",
    "  # divide each element of a row by the sum of that row\n",
    "  return M / M.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = markov_matrix([bach_minuet_melody], encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now transform $\\mathbf{W}$ into a matrix representing probabilities and let us round the entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = tf.exp(W)\n",
    "P = P / tf.reduce_sum(P, axis=1, keepdims=True)\n",
    "P = np.round(P, decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot both matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(P, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(M, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Melody Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.1.1**: The following function ``ffn_score(W, encoder, max_len)`` generates a new ``Score`` based on our computed matrix ``W``. It works similar to ``markov_score(M, encoder, max_len)``.\n",
    "\n",
    "Explain in your own words, what is going on. Try to find out what each operation does. Try to play around with toy examples. For example, you could generate a ``numpy`` matrix to test what ``P = C / np.sum(C, axis=1, keepdims=True)`` compute.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def ffn_score(W, encoder, max_len=100):\n",
    "  score = Score()\n",
    "  part = Part()\n",
    "\n",
    "  C = np.exp(W)\n",
    "  P = C / np.sum(C, axis=1, keepdims=True)\n",
    "  \n",
    "  \n",
    "  for _ in range(max_len):\n",
    "    m = len(encoder)\n",
    "    j = np.random.choice(m, size=1, p=P[j])[0]\n",
    "    symbol = encoder.decode(j)\n",
    "    if symbol == TERM_SYMBOL:\n",
    "      break\n",
    "    else: \n",
    "      part.append(symbol)\n",
    "  score.insert(0, part)\n",
    "  return score\n",
    "\n",
    "ffn_score(W.numpy(), encoder).show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.1.2**: As we saw, using just one matrix to be the neural network--which is in fact the most simplest network we can imagine--we basically learn the Markov matrix.\n",
    "\n",
    "1. Is this method more or less computational expensive compared to our computation of the Markov matrix?\n",
    "2. What do we have to change to consider not only the last note but the last two notes to compute the next one?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(force_save=True, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "otter": {
   "OK_FORMAT": false,
   "assignment_name": "4_1_ml_ffn_markov",
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
