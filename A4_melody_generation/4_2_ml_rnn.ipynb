{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melody Generation\n",
    "\n",
    "+ **AI in Culture and Arts - Tech Crash Course**\n",
    "+ **Date:** 06.06.2024\n",
    "+ **Author:** B. ZÃ¶nnchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/aica-wavelab/aica-assignments/blob/main/A4_melody_generation/4_2_ml_rnn.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will create music sheets and sound. For those tasks ``Python`` requires external programs that you should install if you are working locally:\n",
    "\n",
    "1. [Musescore](https://musescore.org/de) (for generating sheets)\n",
    "2. [FluidSynth](https://www.fluidsynth.org/) (for generating sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on google ``Colab``, you can evaluate the following three cells to install these applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title install dependencies to play sound\n",
    "%%capture\n",
    "print('installing fluidsynth...')\n",
    "!apt-get install fluidsynth > /dev/null\n",
    "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title install dependencies to show score in music notation\n",
    "%%capture\n",
    "print('installing musescore3...')\n",
    "!apt-get install musescore3 > /dev/null\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title clone git repository\n",
    "%%capture\n",
    "%rm -rf aica-assignments\n",
    "!git clone https://github.com/aica-wavelab/aica-assignments.git\n",
    "%cd aica-assignments/A4_melody_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furtheremore, for this notebook we need the following ``Python`` packages and moduls. Execute the cell to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install music21\n",
    "%pip install pyfluidsynth\n",
    "\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "#%pip install tensorflow\n",
    "%pip install tensorflow==2.16.1\n",
    "\n",
    "%pip install otter-grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import music21 as m21\n",
    "\n",
    "import zipfile\n",
    "from files import load_midi_files\n",
    "from pianoroll import stream_to_df, plot_df\n",
    "from encoder import PianoRollEncoder, StringToIntEncoder, TERM_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"4_2_ml_rnn.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Recurrent Neural Network (RNN) / LSTM\n",
    "\n",
    "RNNs allow us to predict the next element in a sequence based on a finite sequence of arbitrary length. How accurate this prediction is, is another question. It works by writing into a sort of *memory* thus it is considered not to be memoryless! Let us assume we have a sequence of notes / events $\\mathbf{x}_0$ = ``C4``, $\\mathbf{x}_1$ = ``G4``, $\\mathbf{x}_2$ =``_``, ... we put first ``C4`` into the RNN. It generates not only an output $\\mathbf{y}_0$, that is, the next predicted note, but also a so-called hidden state $\\mathbf{h}_0$ which acts like a memory. When we input the next note ``G4`` the hidden state is also part of the input and it is also adapted. The following figure shows an RNN unfolded in time.\n",
    "\n",
    "<img src=\"figs/rnn-unfold.png\" alt=\"\" height=\"300\">\n",
    "\n",
    "Since this can go on basically indefinitely, we can consider many notes to predict the next one. In the following, ``sequence_len`` determines the number of notes / events we consider during training. This means we need to ensure that we construct such sequences from the data during data preparation. While our trained RNN / LSTM can also generate longer sequences, it has never seen such sequences before. Therefore, the quality will decrease.\n",
    "\n",
    "Furthermore, this time we will consider events that all have the same duration (one ``time_step`` measured in quarter notes) instead of notes. A note is therefore represented by an ``X-NoteOn`` event followed by a ``NoteHold`` event where ``X`` indicate the pitch or a ``Rest`` event. For example:\n",
    "\n",
    "```\n",
    "65 _ _ _ 77 _ r _ _\n",
    "```\n",
    "\n",
    "This would mean that the piece begins with the MIDI note 65 (``65-NoteOn`` event). The note is held for 3 more time steps (4 in total), then a ``77-NoteOn`` event follows, which is held for one more time step, and finally, a rest follows for a total of 3 time steps. We already showed how to transform a ``Stream`` into such a *piano roll like* represenation using the ``EventEncoder`` class.\n",
    "\n",
    "An **LSTM** is an extension of the vanilla RNN which esentially makes it easier to learn longer sequences. Since we write into one hidden state vector, you can imagine that information gets washed away over time. A vanilla RNN has no ability to learn which kind of information might be more important. An LSTM, which stands for **long short-term memory network** has some ability to learn what to put into its 'long term memory' and what information to put into its 'short term memory'. \n",
    "\n",
    "Do not worry too much about the illustration of the LSTM below. Basically, There are matrices $W_f, W_g, W_i, W_o$ with **learnable** parameters that controls what to forget and what to keep based on the input of the current element of the sequence $\\mathbf{x}_t$ and the last hidden state $\\mathbf{h}_{t-1}$\n",
    "\n",
    "<img src=\"figs/lstm-cell.png\" alt=\"\" height=\"400\">\n",
    "\n",
    "+ **Update Gate:** Determines how much of the new information to add to the cell state $\\mathbf{c}_t$ ('long term memory').\n",
    "+ **Forget Gate:** Decides what information is no longer needed and removes it from the cell state $\\mathbf{c}_t$, helping to prevent the accumulation of irrelevant information.\n",
    "+ **Output Gate:** Controls the extent to which the value in the cell is used to compute the output activation of the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.2.1**: Why would it be impossible to use the *piano roll like* representation for our simple neural network in ``4_1_ml_ffn_markov``?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.1.1 Data Preparation\n",
    "\n",
    "This time we not only use one piece of music but we use roughly 1000 melodies from folk songs. The ``.zip`` file ``data/deu_folk_songs.zip`` contains MIDI files of melodies. These files represent our **training data** for our *deep learning* attempts later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/deu_folk_songs.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/deu_folk_songs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us unzip it. Then you can use the ``load_midi_files`` function to load all the MIDI files. It returns a list of ``Stream`` objects. You can and should specify a ``time_step`` to filter out files that can not be represented by the time step we want to use. We also transpose all the piece into the key of ``C major`` such that our model only has to learn one key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can take a some time!\n",
    "time_step = 0.5\n",
    "streams_folk_songs = load_midi_files('data/deu_folk_songs/', time_step=time_step, transpose_to_major=True) \n",
    "print(f'load {len(streams_folk_songs)} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable a *one-hot encoding* we have to transform this representation into numbers ranging from $0$ to $m-1$, which can be done by using the ``StringToIntEncoder`` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_roll_encoder = PianoRollEncoder(time_step=time_step)\n",
    "piano_rolls, _ = piano_roll_encoder.encode_streams(streams_folk_songs)\n",
    "print(f'piano rolls as strings: {piano_rolls[:3]}')\n",
    "\n",
    "string_to_int = StringToIntEncoder(piano_rolls)\n",
    "piano_rolls_int = string_to_int.encode_sequences(piano_rolls)\n",
    "vocab_size = len(string_to_int)\n",
    "\n",
    "print(f'piano roll as int: {piano_rolls_int[0]}')\n",
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to generate our training dataset. However, this time the input is a sequence of events and the output is the next event!\n",
    "The sequence lenght is our first (hyper-)parameter. To enable the network to be able to predict for shorter sequences we pad with ``TERM_SYMBOL``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 64 # this is a hyper-paremter!\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "i_term_symbol = string_to_int.encode(TERM_SYMBOL)\n",
    "for piano_roll in piano_rolls_int:\n",
    "  padded_piano_roll = [i_term_symbol]*sequence_len + piano_roll + [i_term_symbol]*sequence_len\n",
    "  for i in range(len(padded_piano_roll)-sequence_len):\n",
    "    xs.append(padded_piano_roll[i:(i+sequence_len)])\n",
    "    ys.append(padded_piano_roll[i+sequence_len])\n",
    "\n",
    "print(xs[:3])\n",
    "print(ys[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to transform ``xs`` into a *one-hot encoding*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.one_hot(xs, depth=vocab_size).numpy().astype('float32')\n",
    "y = np.array(ys)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[:3])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidde_state_dimension = 16 # this is a hyper-paremter!\n",
    "learning_rate = 0.01 # this is a hyper-paremter!\n",
    "loss = 'sparse_categorical_crossentropy' # this is a hyper-paremter!\n",
    "epochs = 50 # this is a hyper-paremter!\n",
    "batch_size = 16 # this is a hyper-paremter!\n",
    "save_model_path = f'models/model_{hidde_state_dimension}_{sequence_len}_{batch_size}.keras'\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(None, vocab_size))\n",
    "x = inputs\n",
    "x = tf.keras.layers.LSTM(hidde_state_dimension)(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, output)\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "loss=loss, \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.2.2**: Look at the model summary above. Can you reason about the number of learnable parameters? \n",
    "\n",
    "**Hint:** Look at the image of an LSTM we introduced in the beginning and consider that each of the ``4`` green boxes represents basically a layer of a neural network (a matrix and a bias term). Furthermore, the last number in *Output Shape* of the displayed table displayed when calling ``model.summary()`` is the dimension of the output of that layer of the neural network!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "4*((vocab_size + hidde_state_dimension) * hidde_state_dimension + hidde_state_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidde_state_dimension*vocab_size + vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.1.3 Model Training\n",
    "\n",
    "Becaus the training can take some time, we already trained models using different hyper-paraemters. You can find them in the folder ``models/`` and instead of training, you can load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('cpu:0'):\n",
    "\n",
    "#this can take a while\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# save model\n",
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.2.3**: Find a good metaphor for the hyper-parameter ``hidde_state_dimension``. What might be the effect if we make it larger or smaller?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 4.1.4 (optional) Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A large model with a \"memory size\", that is hidden state of 256 dimenisons\n",
    "path_to_large_model = 'models/model_256_64_128.keras'\n",
    "\n",
    "# A small model with a \"memory size\", that is hidden state of 16 dimenisons\n",
    "path_to_small_model = 'models/model_16_64_16.keras'\n",
    "\n",
    "model = tf.keras.models.load_model(path_to_large_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_melody(model, sequence_len, seed, string_to_int: StringToIntEncoder, temperature: float=1.0, max_len=1_000) -> list[str]:\n",
    "\n",
    "    # Pad with TERM_SYMBOL\n",
    "    padded_seed = [TERM_SYMBOL]*sequence_len + seed\n",
    "    \n",
    "    # Tranform pedding + seed into numbers\n",
    "    seed_int = string_to_int.encode_sequence(padded_seed)\n",
    "\n",
    "    # Add the seed to the melody\n",
    "    melody = seed[:]\n",
    "\n",
    "    while True:\n",
    "        seed_int = seed_int[-sequence_len:]\n",
    "        # One-hot encode the seed\n",
    "        onehot_seed = tf.one_hot([seed_int], depth=len(string_to_int)).numpy().astype('float32')\n",
    "        \n",
    "        # Make prediction, note that we have a softmax-layer integrated into our model\n",
    "        probabilities = model.predict(onehot_seed)[0]\n",
    "\n",
    "        # Because the prediction already applied softmax we have to revert it for including the temperature\n",
    "        probabilities = np.log(probabilities) / temperature \n",
    "\n",
    "        # Recompute softmax afterwards\n",
    "        probabilities = np.exp(probabilities) / np.sum(np.exp(probabilities))\n",
    "\n",
    "        choices = range(len(probabilities))\n",
    "        symbol_int = np.random.choice(choices, p=probabilities)\n",
    "        seed_int.append(symbol_int)\n",
    "        \n",
    "        symbol = string_to_int.decode(symbol_int)\n",
    "\n",
    "        if symbol != TERM_SYMBOL:\n",
    "            melody.append(symbol)\n",
    "            if max_len <= len(melody):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_as_list = generate_melody(model, sequence_len=sequence_len, seed=['60', '_', '_'], string_to_int=string_to_int, temperature=1.0, max_len=100)\n",
    "print(score_as_list)\n",
    "score = piano_roll_encoder.decode_stream(score_as_list)\n",
    "score.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.2.4**: Explain the what the function ``generate_melody`` computes and how it works. Explain what the parameter ``temperature`` controls. You might want to generate multiple melodies with different ``temperature`` values. Listen to the result.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Instruction 4.2.5**: We used all the data for training, neglecting any validation. Explain and think about:\n",
    "\n",
    "1. What is the disadvantage of not validating our model? **Hint:** Think of the concept of *overfitting*.\n",
    "2. What would happen if the model overfits its training data? Is this always bad in our context or do we might want this to happen?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "otter": {
   "assignment_name": "4_2_ml_rnn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
