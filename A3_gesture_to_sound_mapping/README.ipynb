{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and email below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full name\n",
    "NAME = \"\"\n",
    "# Institutional email (hm.edu or hmtm.de)\n",
    "EMAIL = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Mapping gestures to sounds from demonstrations\n",
    "\n",
    "<image src=\"https://live.staticflickr.com/4365/37161246275_7cfcb13035_b.jpg\" style=\"width: 700px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "<figcaption>Photography from Corpus Nil from the artist Marco Donnarumma (IT/DE). The artist is using muscle contraction sensors to generate sounds. Link to the video: <a href=\"https://vimeo.com/152710490\" target=\"_blank\">Corpus Nil</a></figcaption>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The third day of this class will show you:\n",
    "- A novel application of machine learning in culture and art: **mapping gestures to sounds from demonstrations**.\n",
    "- A novel graphical programming environment to develop interactive prototypes: **PureData**\n",
    "\n",
    "Your goal will be to train a machine learning model to generate sounds from gestures. The training data will be collected by you, on spot, using the sensors from your smartphone. \n",
    "\n",
    "Please download the code and data from the [github repository](https://github.com/aica-wavelab/aica-assignments) and follow the instructions in the `A3_gesture_to_sound_mapping`.\n",
    "\n",
    "[Github repository of the course](https://github.com/aica-wavelab/aica-assignments){: .btn .btn-green\n",
    " target=\"_blank\"}\n",
    "\n",
    "## Content of the repository\n",
    "\n",
    "The repository contains the following folders:\n",
    "\n",
    "- `puredata_cheatsheet.pd`: A patch with the main objects seen during the course and their keyboard shortcuts.\n",
    "- `dub_siren_box`: A practical and fun exercise to start apply the concepts seen before. The patch reimplement this [hardware](https://www.youtube.com/watch?v=sCNiYJlU0DA).\n",
    "- `mapping_by_demonstration`: Demonstration of how to map gestures to sound using the ml-lib library.\n",
    "\n",
    "## Assigment\n",
    "\n",
    "Implement a musical instrument using Plugdata and ml-lib. The instrument can map any signal to any sound or sound effect you'd like. You are then assigned to upload the files along with a short video of your instrument in action !\n",
    "\n",
    "## Installation required\n",
    "\n",
    "### Plugdata\n",
    "\n",
    "Plugdata is a free/open-source visual programming environment based on pure-data. It is available for a wide range of operating systems. \n",
    "Please install plugdata for your operating system from the [plugdata website](https://plugdata.org/download.html).\n",
    "\n",
    "### ml-lib\n",
    "\n",
    "ml-lib is a library of machine learning externals for Max and Pure Data designed and developed by [Ali Momeni](http://alimomeni.net) and [Jamie Bullock](http://jamiebullock.com).\n",
    "\n",
    "The goal of ml-lib is to provide a simple, consistent interface to a wide range of machine learning techniques in Max and Pure Data. The canonical NIME 2015 paper on ml-lib can be found [here](https://nime2015.lsu.edu/proceedings/201/0201-paper.pdf).\n",
    "\n",
    "Full class documentation can be found [here](http://irllabs.github.io/ml-lib/).\n",
    "\n",
    "Please download the latest release of ml-lib for your operating system from the [ml-lib website](https://github.com/irllabs/ml-lib/releases/).\n",
    "Then copy the files into the PureData folder.\n",
    "\n",
    "- On Windows, you can copy the files into the folder `C:\\Program Files\\plugdata\\Extra\\ml.lib`\n",
    "- On MacOS, you can copy the files into the folder `Documents/plugdata/Extra/ml.lib`\n",
    "\n",
    "### Streaming data from your phone\n",
    "\n",
    "To stream sensors' data from your phone via the protocol OSC to your computer, you can use the following open-source applications:\n",
    "\n",
    "- On Android: [Sensors2OSC](https://sensors2.org/osc/) can be installed via F-Droid\n",
    "- On iPhone: [Data OSC](https://apps.apple.com/de/app/data-osc/id6447833736?platform=iphone) can be installed via the App Store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
